{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UculQjKRrnW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import jieba.posseg as pseg\n",
        "import jieba.analyse\n",
        "import numpy as np\n",
        "import tensorflow \n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Embedding,LSTM, concatenate, Dense,Dropout,SimpleRNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from scipy.stats import entropy"
      ],
      "metadata": {
        "id": "5GVF1Px011j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI9UJRfKRrnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07dc6234-2619-4bdd-8fb3-bb2b27d595fb"
      },
      "source": [
        "!pip install sklearn-crfsuite\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.8/dist-packages (0.3.6)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (4.64.1)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (0.9.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (0.8.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6LWkA3P8HZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653c628c-f559-49c1-9ba2-49d6b5cc5214"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "folder = '/drive/MyDrive/Courses/AIMAS_2022'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5avpn8FTRrnV"
      },
      "source": [
        "file_path =  \"/content/drive/MyDrive/Courses/AIMAS_2022/sample_data.txt\"\n",
        "\n",
        "with open(file_path, 'r', encoding='utf8') as f:\n",
        "    file_text = f.read().encode('utf-8').decode('utf-8-sig')\n",
        "\n",
        "datas = file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Courses/AIMAS_2022/processed_data.txt\", \"w\") as f:\n",
        "    for article_id, data in enumerate(datas):\n",
        "        data=data.split('\\n')\n",
        "        content=data[0]\n",
        "\n",
        "        annotations=data[1:]\n",
        "        row = list()\n",
        "        for annot in annotations[1:]:\n",
        "            annot=annot.split('\\t') #annot= article_id, start_pos, end_pos, entity_text, entity_type\n",
        "            row.append(annot)\n",
        "\n",
        "        df = pd.DataFrame(row, columns=data[1].split('\\t'))\n",
        "        position_cols = ['start_position', 'end_position']\n",
        "        df[position_cols] = df[position_cols].astype('int')\n",
        "\n",
        "        tmp_label_list = np.array(['O'] * len(content), dtype=object)\n",
        "        for i in range(len(df)):\n",
        "            start, end, etype = df['start_position'][i], df['end_position'][i], df['entity_type'][i]\n",
        "            # print(start, end, etype)\n",
        "            tmp_label_list[start] = \"B-\" + str(etype)\n",
        "            tmp_label_list[start+1:end] = \"I-\" + str(etype)\n",
        "\n",
        "        for i, row in enumerate(zip(list(content), tmp_label_list)):\n",
        "            f.write(\" \".join(row) + '\\n')\n",
        "        \n",
        "        f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GMS2hCoRrnW"
      },
      "source": [
        "# def CRF(x_train, y_train, x_test, y_test):\n",
        "#     # Doc: https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#module-sklearn_crfsuite\n",
        "#     # crf = sklearn_crfsuite.CRF(\n",
        "#     #     algorithm='lbfgs',\n",
        "#     #     c1=0.1,\n",
        "#     #     c2=0.1,\n",
        "#     #     max_iterations=100,\n",
        "#     #     all_possible_transitions=True,\n",
        "#     # )\n",
        "#     # crf.fit(x_train, y_train)\n",
        "    \n",
        "#     y_pred = crf.predict(x_test)\n",
        "#     y_pred_mar = crf.predict_marginals(x_test)\n",
        "\n",
        "#     labels = list(crf.classes_)\n",
        "#     labels.remove('O')\n",
        "#     f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
        "#     sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
        "\n",
        "#     return y_pred, y_pred_mar, f1score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHtTSi_8RrnW"
      },
      "source": [
        "# dim = 0\n",
        "# word_vecs= {}\n",
        "# with open(\"/content/drive/MyDrive/Courses/AIMAS_2022/cna.cbow.cwe_p.tar_g.512d.0.txt\") as f:\n",
        "#     for line in f:\n",
        "#         tokens = line.strip().split()\n",
        "\n",
        "#         if len(tokens) == 2:\n",
        "#             dim = int(tokens[1])\n",
        "#             continue\n",
        "    \n",
        "#         word = tokens[0] \n",
        "#         vec = np.array([ float(t) for t in tokens[1:] ])\n",
        "#         word_vecs[word] = vec\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ey0U_XhpRrnW"
      },
      "source": [
        "# print(f'vocabulary_size: {len(word_vecs)}')\n",
        "# print(f'word_vector_dim: {vec.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySlER2ErRrnW"
      },
      "source": [
        "def make_dataset(data_path):\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        data = f.readlines() \n",
        "    data_list, data_list_tmp = list(), list()\n",
        "    article_id_list = list()\n",
        "    idx = 0\n",
        "    for row in data:\n",
        "        data_tuple = tuple()\n",
        "        if row == '\\n':\n",
        "            article_id_list.append(idx)\n",
        "            idx+=1\n",
        "            data_list.append(data_list_tmp)\n",
        "            data_list_tmp = []\n",
        "        else:\n",
        "            row = row.strip('\\n').split(' ')\n",
        "            data_tuple = (row[0], row[1])\n",
        "            data_list_tmp.append(data_tuple)\n",
        "    if len(data_list_tmp) != 0:\n",
        "        data_list.append(data_list_tmp)\n",
        "    \n",
        "    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = train_test_split(data_list,article_id_list,test_size=0.33,random_state=42)\n",
        "     \n",
        "    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3P8vbYGRrnX"
      },
      "source": [
        "def build_word_vectors(data_list, embedding_dict):\n",
        "    embedding_list = list()\n",
        "\n",
        "    unk_vector = np.random.rand(*(list(embedding_dict.values())[0].shape))\n",
        "\n",
        "    for idx_list in range(len(data_list)):\n",
        "        embedding_list_tmp = list()\n",
        "        for idx_tuple in range(len(data_list[idx_list])):\n",
        "            key = data_list[idx_list][idx_tuple][0] # token\n",
        "\n",
        "            if key in embedding_dict:\n",
        "                value = embedding_dict[key]\n",
        "            else:\n",
        "                value = unk_vector\n",
        "            embedding_list_tmp.append(value)\n",
        "        embedding_list.append(embedding_list_tmp)\n",
        "        \n",
        "    return embedding_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNa4fSkYRrnX"
      },
      "source": [
        "def process_labels(data_list):\n",
        "    label_list = list()\n",
        "    for idx_list in range(len(data_list)):\n",
        "        label_list_tmp = list()\n",
        "        for idx_tuple in range(len(data_list[idx_list])):\n",
        "            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n",
        "        label_list.append(label_list_tmp)\n",
        "        \n",
        "    return label_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzmKa42yRrnX"
      },
      "source": [
        "def make_features(embed_list):\n",
        "    feature_list = list()\n",
        "    for idx_list in range(len(embed_list)):\n",
        "        feature_list_tmp = list()\n",
        "        for idx_tuple in range(len(embed_list[idx_list])):\n",
        "            # feature_dict = dict()\n",
        "            feature_dict=[]\n",
        "            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n",
        "                feature_dict.append(embed_list[idx_list][idx_tuple][idx_vec])\n",
        "                # feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n",
        "            feature_list_tmp.append(feature_dict)\n",
        "        feature_list.append(feature_list_tmp)\n",
        "\n",
        "    return feature_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "      \n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "metadata": {
        "id": "Dop36ZmyDtg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_uGdFy3RrnX"
      },
      "source": [
        "data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = make_dataset(\"/content/drive/MyDrive/Courses/AIMAS_2022/processed_data.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listx=[]\n",
        "listy=[]\n",
        "lists=[]\n",
        "\n",
        "longmax=0\n",
        "index=0\n",
        "now=0\n",
        "de=0\n",
        "\n",
        "for y in traindata_list+testdata_list:\n",
        "  templong=0\n",
        "  for x in y:\n",
        "    templong+=1\n",
        "    p=\"\"\"！？｡：；～、…，\"\"\"\n",
        "    if(x[0] not in p):\n",
        "      listx.append(x[0])\n",
        "      listy.append(x[1])\n",
        "      lists.append('Sentence: '+str(now))\n",
        "    if(x[0] in p):\n",
        "      if(x[0]=='…'):\n",
        "        if(de == 1):\n",
        "          de=0\n",
        "          continue\n",
        "        else:\n",
        "          de=1  \n",
        "      now+=1\n",
        "      if(templong>longmax):\n",
        "        longmax=templong\n",
        "        index=now\n",
        "      templong=0\n",
        "\n",
        "print(now)\n",
        "print(longmax)"
      ],
      "metadata": {
        "id": "rdl5N2WcCip4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ec3ded-35b8-4755-9e9b-cd656c162390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5649\n",
            "62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(list(zip(lists,listx,listy)),columns =['Sentence #','Word','Tag',])\n",
        "data"
      ],
      "metadata": {
        "id": "c0ZaYMZ3DaLm",
        "outputId": "714cf8a4-e2ae-4c08-8681-0d2627ab703d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Sentence # Word Tag\n",
              "0         Sentence: 0    醫   O\n",
              "1         Sentence: 0    師   O\n",
              "2         Sentence: 1    阿   O\n",
              "3         Sentence: 1    嬤   O\n",
              "4         Sentence: 1    回   O\n",
              "...               ...  ...  ..\n",
              "41037  Sentence: 5649    就   O\n",
              "41038  Sentence: 5649    這   O\n",
              "41039  Sentence: 5649    樣   O\n",
              "41040  Sentence: 5649    囉   O\n",
              "41041  Sentence: 5649    。   O\n",
              "\n",
              "[41042 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a361fe54-794f-4917-a4c3-506c58ae6f62\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 0</td>\n",
              "      <td>醫</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 0</td>\n",
              "      <td>師</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>阿</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>嬤</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>回</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41037</th>\n",
              "      <td>Sentence: 5649</td>\n",
              "      <td>就</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41038</th>\n",
              "      <td>Sentence: 5649</td>\n",
              "      <td>這</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41039</th>\n",
              "      <td>Sentence: 5649</td>\n",
              "      <td>樣</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41040</th>\n",
              "      <td>Sentence: 5649</td>\n",
              "      <td>囉</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41041</th>\n",
              "      <td>Sentence: 5649</td>\n",
              "      <td>。</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41042 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a361fe54-794f-4917-a4c3-506c58ae6f62')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a361fe54-794f-4917-a4c3-506c58ae6f62 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a361fe54-794f-4917-a4c3-506c58ae6f62');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "count_class = Counter(data[\"Word\"])"
      ],
      "metadata": {
        "id": "P7PSoIQnmyKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(set(data[\"Word\"].values))\n",
        "words.append(\"ENDPAD\")\n",
        "n_words = len(words) \n",
        "print(n_words)\n",
        "tags = list(set(data[\"Tag\"].values))\n",
        "n_tags = len(tags)\n",
        "print(n_tags)"
      ],
      "metadata": {
        "id": "1tqvI1H0D1yy",
        "outputId": "e6aa1e15-d84e-4f8b-db8b-7ef9d0ab3444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1225\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getter = SentenceGetter(data)\n",
        "sent = getter.get_next() \n",
        "sentences = getter.sentences\n",
        "len(sentences)"
      ],
      "metadata": {
        "id": "ss4r3WciDvAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b8c03c-f658-482c-dc9e-3731cd24be0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5647"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.parsers.base_parser import is_index_col\n",
        "ss=[]\n",
        "m=0\n",
        "ii=0\n",
        "si=0\n",
        "b=35\n",
        "bb=0\n",
        "for s in sentences:\n",
        "  ii+=1\n",
        "  if(len(s)>m):\n",
        "    m=len(s)\n",
        "    si=ii\n",
        "  if(len(s)>b):\n",
        "    bb+=1\n",
        "  ss.append(len(s))\n",
        "\n",
        "ss=np.array(ss)\n",
        "print(np.mean(ss))\n",
        "print(np.max(ss))\n",
        "print(np.min(ss))\n",
        "# print(si)\n",
        "print(bb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKqbakDtOmhk",
        "outputId": "364d9d16-9cb8-4a24-abb8-4aa4bb347b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.267929874269524\n",
            "61\n",
            "1\n",
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "metadata": {
        "id": "hiROYyosF6n2",
        "outputId": "f368e9d2-f670-4abc-cf28-e72bece3eb5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-cc4a02ha\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-cc4a02ha\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (from keras-contrib==2.0.8) (2.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "daAQM4qTyFvT",
        "outputId": "9ade9d12-c0be-49e6-db67-ee708a55727d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.8/dist-packages (0.19.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as L\n",
        "from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
        "\n",
        "\n",
        "class CRF(L.Layer):\n",
        "    def __init__(self,\n",
        "                 output_dim,\n",
        "                 sparse_target=True,\n",
        "                 **kwargs):\n",
        "        \"\"\"    \n",
        "        Args:\n",
        "            output_dim (int): the number of labels to tag each temporal input.\n",
        "            sparse_target (bool): whether the the ground-truth label represented in one-hot.\n",
        "        Input shape:\n",
        "            (batch_size, sentence length, output_dim)\n",
        "        Output shape:\n",
        "            (batch_size, sentence length, output_dim)\n",
        "        \"\"\"\n",
        "        super(CRF, self).__init__(**kwargs)\n",
        "        self.output_dim = int(output_dim) \n",
        "        self.sparse_target = sparse_target\n",
        "        self.input_spec = L.InputSpec(min_ndim=3)\n",
        "        self.supports_masking = False\n",
        "        self.sequence_lengths = None\n",
        "        self.transitions = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        f_shape = tf.TensorShape(input_shape)\n",
        "        input_spec = L.InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n",
        "\n",
        "        if f_shape[-1] is None:\n",
        "            raise ValueError('The last dimension of the inputs to `CRF` '\n",
        "                             'should be defined. Found `None`.')\n",
        "        if f_shape[-1] != self.output_dim:\n",
        "            raise ValueError('The last dimension of the input shape must be equal to output'\n",
        "                             ' shape. Use a linear layer if needed.')\n",
        "        self.input_spec = input_spec\n",
        "        self.transitions = self.add_weight(name='transitions',\n",
        "                                           shape=[self.output_dim, self.output_dim],\n",
        "                                           initializer='glorot_uniform',\n",
        "                                           trainable=True)\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "    def call(self, inputs, sequence_lengths=None, training=None, **kwargs):\n",
        "        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
        "        if sequence_lengths is not None:\n",
        "            assert len(sequence_lengths.shape) == 2\n",
        "            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n",
        "            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n",
        "            assert seq_len_shape[1] == 1\n",
        "            self.sequence_lengths = K.flatten(sequence_lengths)\n",
        "        else:\n",
        "            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n",
        "                tf.shape(inputs)[1]\n",
        "            )\n",
        "\n",
        "        viterbi_sequence, _ = crf_decode(sequences,\n",
        "                                         self.transitions,\n",
        "                                         self.sequence_lengths)\n",
        "        output = K.one_hot(viterbi_sequence, self.output_dim)\n",
        "        return K.in_train_phase(sequences, output)\n",
        "\n",
        "    @property\n",
        "    def loss(self):\n",
        "        def crf_loss(y_true, y_pred):\n",
        "            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
        "            log_likelihood, self.transitions = crf_log_likelihood(\n",
        "                y_pred,\n",
        "                tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n",
        "                self.sequence_lengths,\n",
        "                transition_params=self.transitions,\n",
        "            )\n",
        "            return tf.reduce_mean(-log_likelihood)\n",
        "        return crf_loss\n",
        "\n",
        "    @property\n",
        "    def accuracy(self):\n",
        "        def viterbi_accuracy(y_true, y_pred):\n",
        "            mask = K.cast(\n",
        "                K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
        "            shape = tf.shape(y_pred)\n",
        "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
        "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
        "            if self.sparse_target:\n",
        "                y_true = K.argmax(y_true, 2)\n",
        "            y_pred = K.cast(y_pred, 'int32')\n",
        "            y_true = K.cast(y_true, 'int32')\n",
        "            corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
        "            return K.sum(corrects * mask) / K.sum(mask)\n",
        "        return viterbi_accuracy\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        tf.TensorShape(input_shape).assert_has_rank(3)\n",
        "        return input_shape[:2] + (self.output_dim,)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'sparse_target': self.sparse_target,\n",
        "            'supports_masking': self.supports_masking,\n",
        "            'transitions': K.eval(self.transitions)\n",
        "        }\n",
        "        base_config = super(CRF, self).get_config()"
      ],
      "metadata": {
        "id": "faZyoexornOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEQBFzYw8fqU",
        "outputId": "ef115589-b01c-4804-85f3-1f10da31715a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-time',\n",
              " 'I-time',\n",
              " 'I-location',\n",
              " 'I-med_exam',\n",
              " 'B-location',\n",
              " 'I-name',\n",
              " 'B-profession',\n",
              " 'B-organization',\n",
              " 'I-money',\n",
              " 'I-organization',\n",
              " 'B-med_exam',\n",
              " 'B-money',\n",
              " 'O',\n",
              " 'B-name',\n",
              " 'I-profession']"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 37\n",
        "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words-1)\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
        "from keras.utils import to_categorical\n",
        "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.33,random_state=2023)"
      ],
      "metadata": {
        "id": "IGgoVOPWFut7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx2tag = {i: w for w, i in tag2idx.items()}\n",
        "\n",
        "def pred2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            p_i = np.argmax(p)\n",
        "            out_i.append(idx2tag[p_i])\n",
        "        out.append(out_i)\n",
        "    return out"
      ],
      "metadata": {
        "id": "MayP2wTSlI2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "\n",
        "num_labels = 15\n",
        "crf = CRF(num_labels, sparse_target=True)\n",
        "adam = tensorflow.keras.optimizers.Adam(lr=0.005)\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=n_words+1, output_dim=200, input_length=max_len))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
        "model.add(crf)\n",
        "model.compile('adam', loss=crf.loss, metrics=[crf.accuracy])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "riaiEVKArt0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d345ed7-afbc-475d-d610-60b90352d20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 37, 200)           245200    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 37, 200)           0         \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 37, 256)          336896    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 37, 15)           3855      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " crf_3 (CRF)                 (None, 37, 15)            225       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 586,176\n",
            "Trainable params: 586,176\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=np.array(X_tr)\n",
        "y_train=np.array(y_tr)\n",
        "x_train = np.asarray(x_train,dtype='float32')\n",
        "y_train = np.asarray(y_train,dtype='float32')\n",
        "model.fit(x_train, y_train, batch_size=16, epochs=50, verbose=1)"
      ],
      "metadata": {
        "id": "1uDTSbq0GN-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7892d8-69f7-473f-d610-87d01c8b7a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "237/237 [==============================] - 47s 159ms/step - loss: 4.0048 - viterbi_accuracy: 0.9850\n",
            "Epoch 2/50\n",
            "237/237 [==============================] - 27s 116ms/step - loss: 0.8474 - viterbi_accuracy: 0.9946\n",
            "Epoch 3/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.6964 - viterbi_accuracy: 0.9951\n",
            "Epoch 4/50\n",
            "237/237 [==============================] - 28s 119ms/step - loss: 0.5701 - viterbi_accuracy: 0.9958\n",
            "Epoch 5/50\n",
            "237/237 [==============================] - 31s 129ms/step - loss: 0.4796 - viterbi_accuracy: 0.9963\n",
            "Epoch 6/50\n",
            "237/237 [==============================] - 28s 119ms/step - loss: 0.4265 - viterbi_accuracy: 0.9965\n",
            "Epoch 7/50\n",
            "237/237 [==============================] - 28s 120ms/step - loss: 0.3961 - viterbi_accuracy: 0.9966\n",
            "Epoch 8/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.3536 - viterbi_accuracy: 0.9968\n",
            "Epoch 9/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.3205 - viterbi_accuracy: 0.9970\n",
            "Epoch 10/50\n",
            "237/237 [==============================] - 28s 118ms/step - loss: 0.2962 - viterbi_accuracy: 0.9972\n",
            "Epoch 11/50\n",
            "237/237 [==============================] - 30s 125ms/step - loss: 0.2742 - viterbi_accuracy: 0.9973\n",
            "Epoch 12/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.2527 - viterbi_accuracy: 0.9976\n",
            "Epoch 13/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.2410 - viterbi_accuracy: 0.9975\n",
            "Epoch 14/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.2141 - viterbi_accuracy: 0.9977\n",
            "Epoch 15/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.2046 - viterbi_accuracy: 0.9979\n",
            "Epoch 16/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.1884 - viterbi_accuracy: 0.9979\n",
            "Epoch 17/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.1784 - viterbi_accuracy: 0.9981\n",
            "Epoch 18/50\n",
            "237/237 [==============================] - 30s 125ms/step - loss: 0.1838 - viterbi_accuracy: 0.9981\n",
            "Epoch 19/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.1543 - viterbi_accuracy: 0.9984\n",
            "Epoch 20/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.1350 - viterbi_accuracy: 0.9986\n",
            "Epoch 21/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.1239 - viterbi_accuracy: 0.9986\n",
            "Epoch 22/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.1216 - viterbi_accuracy: 0.9985\n",
            "Epoch 23/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.1071 - viterbi_accuracy: 0.9987\n",
            "Epoch 24/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.1047 - viterbi_accuracy: 0.9987\n",
            "Epoch 25/50\n",
            "237/237 [==============================] - 30s 125ms/step - loss: 0.1037 - viterbi_accuracy: 0.9987\n",
            "Epoch 26/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.0869 - viterbi_accuracy: 0.9989\n",
            "Epoch 27/50\n",
            "237/237 [==============================] - 27s 116ms/step - loss: 0.0808 - viterbi_accuracy: 0.9990\n",
            "Epoch 28/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.0767 - viterbi_accuracy: 0.9990\n",
            "Epoch 29/50\n",
            "237/237 [==============================] - 27s 116ms/step - loss: 0.0733 - viterbi_accuracy: 0.9992\n",
            "Epoch 30/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.0734 - viterbi_accuracy: 0.9991\n",
            "Epoch 31/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.0661 - viterbi_accuracy: 0.9993\n",
            "Epoch 32/50\n",
            "237/237 [==============================] - 30s 125ms/step - loss: 0.0680 - viterbi_accuracy: 0.9992\n",
            "Epoch 33/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.0822 - viterbi_accuracy: 0.9990\n",
            "Epoch 34/50\n",
            "237/237 [==============================] - 27s 115ms/step - loss: 0.0607 - viterbi_accuracy: 0.9992\n",
            "Epoch 35/50\n",
            "237/237 [==============================] - 27s 116ms/step - loss: 0.0583 - viterbi_accuracy: 0.9992\n",
            "Epoch 36/50\n",
            "237/237 [==============================] - 27s 116ms/step - loss: 0.0538 - viterbi_accuracy: 0.9992\n",
            "Epoch 37/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.0510 - viterbi_accuracy: 0.9993\n",
            "Epoch 38/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.0440 - viterbi_accuracy: 0.9994\n",
            "Epoch 39/50\n",
            "237/237 [==============================] - 30s 125ms/step - loss: 0.0443 - viterbi_accuracy: 0.9994\n",
            "Epoch 40/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.0411 - viterbi_accuracy: 0.9994\n",
            "Epoch 41/50\n",
            "237/237 [==============================] - 28s 116ms/step - loss: 0.0384 - viterbi_accuracy: 0.9995\n",
            "Epoch 42/50\n",
            "237/237 [==============================] - 27s 116ms/step - loss: 0.0604 - viterbi_accuracy: 0.9992\n",
            "Epoch 43/50\n",
            "237/237 [==============================] - 27s 116ms/step - loss: 0.0412 - viterbi_accuracy: 0.9994\n",
            "Epoch 44/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.0380 - viterbi_accuracy: 0.9994\n",
            "Epoch 45/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.0355 - viterbi_accuracy: 0.9994\n",
            "Epoch 46/50\n",
            "237/237 [==============================] - 29s 123ms/step - loss: 0.0337 - viterbi_accuracy: 0.9994\n",
            "Epoch 47/50\n",
            "237/237 [==============================] - 28s 119ms/step - loss: 0.0318 - viterbi_accuracy: 0.9995\n",
            "Epoch 48/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.0308 - viterbi_accuracy: 0.9995\n",
            "Epoch 49/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.0417 - viterbi_accuracy: 0.9994\n",
            "Epoch 50/50\n",
            "237/237 [==============================] - 28s 117ms/step - loss: 0.0327 - viterbi_accuracy: 0.9994\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a6c179ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = model.predict(X_te, verbose=1)\n",
        "\n",
        "pred_labels = pred2label(test_pred)\n",
        "test_labels = pred2label(y_te)\n",
        "\n",
        "labels = list(tags)\n",
        "labels.remove('O')\n",
        "\n",
        "f1score = metrics.flat_f1_score(test_labels, pred_labels, average='weighted', labels=labels)\n",
        "f1score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9054hjv5VIIc",
        "outputId": "f6b1ea52-3b17-4d99-f9bd-90eaf0f45a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 4s 53ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6997282722069188"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(0,50):\n",
        "#   # i = 50\n",
        "#   p = model.predict(np.array([X_te[i]]))\n",
        "#   p = np.argmax(p, axis=-1)\n",
        "#   true = np.argmax(y_te[i], -1)\n",
        "#   print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
        "#   print(30 * \"=\")\n",
        "#   for w, t, pred in zip(X_te[i], true, p[0]):\n",
        "#       if w != 0:\n",
        "#           print(\"{:15}: {:5} {}\".format(words[w-1], tags[t], tags[pred]))"
      ],
      "metadata": {
        "id": "iXbBiyfllXz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_tags=15\n",
        "# inputs = Input(shape=(None,), dtype='int32')\n",
        "# output = Embedding(input_dim=n_words+1, output_dim=200, input_length=max_len)(inputs)\n",
        "# output = Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1))(output)\n",
        "# output = TimeDistributed(Dense(n_tags, activation=\"relu\"))(output)\n",
        "# crf = tfa.layers.CRF(n_tags)\n",
        "# decoded_sequence, potentials, sequence_length, chain_kernel = crf(output) \n",
        "# model = Model(inputs, potentials)\n",
        "# model.compile(optimizer=\"adam\", loss=crf.add_loss, metrics=['accuracy'])\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(input_dim=n_words+1, output_dim=200, input_length=max_len))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
        "# model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
        "# crf_layer = tfa.layers.CRF(n_tags)\n",
        "# model.add(crf_layer)"
      ],
      "metadata": {
        "id": "WxQvjcAZGAkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "metadata": {
        "id": "Xbs4GHMD1Rac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# class Data_set:\n",
        "#     def __init__(self,data_path,labels):\n",
        "#         with open(data_path,\"rb\") as f:\n",
        "#             self.data = f.read().decode(\"utf-8\")\n",
        "#         self.process_data = self.process_data()\n",
        "#         self.labels = labels\n",
        "#     def process_data(self):\n",
        "#         train_data =self.data.split(\"\\n\\n\")\n",
        "#         train_data = [token.split(\"\\n\") for token in train_data]\n",
        "#         train_data = [[j.split() for j in i ] for i in train_data]\n",
        "#         train_data.pop()\n",
        "#         return train_data\n",
        "    \n",
        "#     def save_vocab(self,save_path):\n",
        "#         all_char = [ char[0] for sen in self.process_data for char in sen]\n",
        "#         chars = set(all_char)\n",
        "#         word2id = {char:id_+1 for id_,char in enumerate(chars)}\n",
        "#         word2id[\"unk\"] = 0\n",
        "#         with open (save_path,\"wb\") as f:\n",
        "#             pickle.dump(word2id,f)\n",
        "#         return word2id    \n",
        "    \n",
        "#     def generate_data(self,vocab,maxlen):\n",
        "#         char_data_sen = [[token[0] for token in i ] for i in self.process_data]\n",
        "#         label_sen = [[token[1] for token in i ] for i in self.process_data]\n",
        "#         sen2id = [[ vocab.get(char,0) for char in sen] for sen in char_data_sen]\n",
        "#         label2id = {label:id_ for id_,label in enumerate(self.labels)}\n",
        "#         lab_sen2id = [[label2id.get(lab,0) for lab in sen] for sen in label_sen]\n",
        "#         sen_pad = pad_sequences(sen2id,maxlen)\n",
        "#         lab_pad = pad_sequences(lab_sen2id,maxlen,value=-1)\n",
        "#         lab_pad = np.expand_dims(lab_pad, 2)\n",
        "#         return sen_pad ,lab_pad"
      ],
      "metadata": {
        "id": "FRcbmcf1a34Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow_addons\n",
        "# from tensorflow_addons.layer import CRF\n",
        "# !pip install tf2crf"
      ],
      "metadata": {
        "id": "5tSq3cFvyhDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras import Sequential\n",
        "# from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "# from keras_contrib.layers import CRF\n",
        "# from keras_contrib.losses import crf_loss\n",
        "# from keras_contrib.metrics import crf_accuracy\n",
        "\n",
        "# class Ner:\n",
        "#     def __init__(self,vocab,labels_category,Embedding_dim=200):\n",
        "#         self.Embedding_dim = Embedding_dim\n",
        "#         self.vocab = vocab\n",
        "#         self.labels_category = labels_category\n",
        "#         self.model = self.build_model()\n",
        "\n",
        "\n",
        "#     def build_model(self):\n",
        "#         inputs = Input(shape=(None,), dtype='int32')\n",
        "#         output = Embedding(100, 40, trainable=True, mask_zero=False)(inputs)\n",
        "#         output = Bidirectional(GRU(64, return_sequences=True))(output)\n",
        "#         output = Dense(9, activation=None)(output)\n",
        "#         crf = CRF(dtype='float32')\n",
        "#         output = crf(output)\n",
        "#         model = Model(inputs, output)\n",
        "#         model.compile(loss=crf.loss, optimizer='adam', metrics=[crf.accuracy])\n",
        "#         # n_words=len(x_train)\n",
        "#         # max_len=75\n",
        "#         # n_tags=11\n",
        "#         # model = Sequential()\n",
        "#         # model.add(Embedding(input_dim=n_words+1, output_dim=200, input_length=max_len))\n",
        "#         # model.add(Dropout(0.5))\n",
        "#         # model.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
        "#         # model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
        "#         # crf_layer = CRF(n_tags)\n",
        "#         # model.add(crf_layer)\n",
        "#         # model.summary()\n",
        "#         # model.compile(optimizer='adam', loss=crf_layer.loss_function, metrics=[crf_layer.accuracy])\n",
        "#         return model\n",
        "\n",
        "#     def train(self,data,label,EPOCHS):\n",
        "#         self.model.fit(data,label,batch_size=16,epochs=EPOCHS)\n",
        "#         self.model.save('crf.h5')\n",
        "        \n",
        "#     def predict(self,model_path,data,maxlen):\n",
        "#         model =self.model\n",
        "#         char2id = [self.vocab.get(i) for i in data]\n",
        "#         pad_num = maxlen - len(char2id)\n",
        "#         input_data = pad_sequences([char2id],maxlen)\n",
        "#         model.load_weights(model_path)\n",
        "#         result = model.predict(input_data)[0][-len(data):]\n",
        "#         result_label = [np.argmax(i) for i in result]\n",
        "#         return result_label"
      ],
      "metadata": {
        "id": "yeDPDVDde4Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tf2crf import CRF\n",
        "# from tensorflow.keras.layers import Input, Embedding, Bidirectional, GRU, Dense\n",
        "# from tensorflow.keras.models import Model\n",
        "\n",
        "# inputs = Input(shape=(None,), dtype='int32')\n",
        "# output = Embedding(100, 40, trainable=True, mask_zero=True)(inputs)\n",
        "# output = Bidirectional(GRU(64, return_sequences=True))(output)\n",
        "# output = Dense(9, activation=None)(output)\n",
        "# crf = CRF(dtype='float32')\n",
        "# output = crf(output)\n",
        "# model = Model(inputs, output)\n",
        "# model.compile(loss=crf.loss, optimizer='adam', metrics=[crf.accuracy])\n",
        "\n",
        "# x = [[5, 2, 3] * 3] * 10\n",
        "# y = [[1, 2, 3] * 3] * 10\n",
        "\n",
        "# model.fit(x=x, y=y, epochs=2, batch_size=2)\n",
        "# model.save('model')"
      ],
      "metadata": {
        "id": "7f8ARmyc8zdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tags = ['B-location','I-location','B-time','I-time','B-med_exam','I-med_exam','B-name','I-name','B-money','I-money','O','B-profession',\n",
        "#     'I-profession','B-organization','I-organization']\n",
        "# data = Data_set(\"/content/drive/MyDrive/Courses/AIMAS_2022/processed_data.txt\",tags)\n",
        "# vocab = data.save_vocab(\"vocab.pk\")\n",
        "# sentence,sen_tags= data.generate_data(vocab,75)"
      ],
      "metadata": {
        "id": "g9ILn-r1eqye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ner = Ner(vocab,tags)"
      ],
      "metadata": {
        "id": "8VrdJf--spau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ner.train(sentence,sen_tags,1)"
      ],
      "metadata": {
        "id": "ETcpcz5upY8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainembed_list = build_word_vectors(traindata_list, word_vecs)\n",
        "# testembed_list = build_word_vectors(testdata_list, word_vecs)\n",
        "\n",
        "# x_train = make_features(trainembed_list)\n",
        "# y_train = process_labels(traindata_list)\n",
        "\n",
        "# x_test = make_features(testembed_list)\n",
        "# y_test = process_labels(testdata_list)\n",
        "\n",
        "# x=[]\n",
        "# for xt in x_train:\n",
        "#   for xxt in xt:\n",
        "#     x.append(xxt)\n",
        "# y=[]\n",
        "# for xt in y_train:\n",
        "#   for xxt in xt:\n",
        "#     y.append(xxt)\n",
        "\n",
        "# x_train=x\n",
        "# y_train=y \n",
        "\n",
        "\n",
        "# x=[]\n",
        "# for xt in x_test:\n",
        "#   for xxt in xt:\n",
        "#     x.append(xxt)\n",
        "# y=[]\n",
        "# for xt in y_test:\n",
        "#   for xxt in xt:\n",
        "#     y.append(xxt)\n",
        "\n",
        "# x_test=x\n",
        "# y_test=y \n",
        "\n",
        "# print(len(x_train))\n",
        "# print(len(y_train))\n",
        "# model = Sequential()\n",
        "\n",
        "# MAX_NUM_WORDS = 360000\n",
        "# MAX_SEQUENCE_LENGTH = 1000000\n",
        "# NUM_CLASSES = 11\n",
        "# NUM_EMBEDDING_DIM = 512\n",
        "# NUM_LSTM_UNITS =512\n",
        "# # import pickle\n",
        "# import numpy as np\n",
        "# from keras import Sequential\n",
        "# from keras_contrib.layers import CRF\n",
        "# import pickle\n",
        "# from keras.layers import Embedding ,Bidirectional,LSTM\n",
        "\n",
        "# model = Sequential()\n",
        "\n",
        "# model.add(Embedding(30000,64))  # Random embedding\n",
        "\n",
        "# model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "\n",
        "# # # crf = CRF(11, sparse_target=True)\n",
        "# # # model.add(crf)\n",
        "# # model.summary()\n",
        "# # model.compile('adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
        "\n",
        "# # model.add(Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM,trainable=False))\n",
        "\n",
        "# # model.add(LSTM(NUM_LSTM_UNITS))\n",
        "\n",
        "# model.add(Dense(units=NUM_CLASSES,activation='softmax'))\n",
        "\n",
        "# print(model.summary())\n",
        "\n",
        "# model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# label_to_index = {\n",
        "#     'B-location':0,\n",
        "#     'I-location':1,\n",
        "#     'B-time':2,\n",
        "#     'I-time':3,\n",
        "#     'B-med_exam':4,\n",
        "#     'I-med_exam':5,\n",
        "#     'B-name':6,\n",
        "#     'I-name':7,\n",
        "#     'B-money':8,\n",
        "#     'I-money':9,\n",
        "#     'O':10,\n",
        "#     # 'B-profession':10,\n",
        "#     # 'I-profession':10,\n",
        "#     # 'B-organization':10,\n",
        "#     # 'I-organization':10,\n",
        "\n",
        "#   }\n",
        "\n",
        "# BATCH_SIZE = 64\n",
        "# NUM_EPOCHS = 5\n",
        "\n",
        "# y_train= pd.DataFrame(y_train,columns =['Names'])\n",
        "# y_train = y_train.Names.apply(lambda x: label_to_index[x])\n",
        "# y_train =tensorflow.keras.utils.to_categorical(y_train)\n",
        "# y_train = np.asarray(y_train)\n",
        "\n",
        "\n",
        "# x_train = np.asarray(x_train)\n",
        "\n",
        "# print(len(x_train))\n",
        "# print(len(y_train))\n",
        "\n",
        "# # y_test=pd.DataFrame(y_test,columns =['Names'])\n",
        "# # y_test = y_test.Names.apply(lambda x: label_to_index[x])\n",
        "# # store=y_test\n",
        "# # y_test =tensorflow.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# # x_test = np.asarray(x_test)\n",
        "# # y_test = np.asarray(y_test)\n",
        "\n",
        "# history = model.fit(\n",
        "#     x=x_train,\n",
        "#     y=y_train,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     epochs=NUM_EPOCHS,\n",
        "#     # validation_data=(x_test,y_test),\n",
        "#     shuffle=False,\n",
        "#     verbose=1\n",
        "# ) \n",
        "\n",
        "# # y_test=store"
      ],
      "metadata": {
        "id": "z-reY4cw2Cxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_test=pd.DataFrame(y_test[0],columns =['Names'])\n",
        "# y_test = y_test.Names.apply(lambda x: label_to_index[x])\n",
        "# x_test = np.asarray(x_test[0])\n",
        "# y_pred = model.predict(x_test)\n",
        "# y_pred=np.argmax(y_pred,axis=1)"
      ],
      "metadata": {
        "id": "4QbeXPBEKi4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import f1_score\n",
        "# labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "# f1_score(y_test, y_pred,average='weighted',labels=labels)"
      ],
      "metadata": {
        "id": "7nZSevJjLVF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index_to_label = {v: k for k, v in label_to_index.items()}\n",
        "\n",
        "# output = [index_to_label[idx] for idx in np.argmax(y_pred, axis=1)]"
      ],
      "metadata": {
        "id": "fAMiGb1uKtXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC66RB5tRrnX"
      },
      "source": [
        "# output=\"article_id\\tstart_position\\tend_position\\tentity_text\\tentity_type\\n\"\n",
        "# for test_id in range(len(y_pred)):\n",
        "#     pos=0\n",
        "#     start_pos=None\n",
        "#     end_pos=None\n",
        "#     entity_text=None\n",
        "#     entity_type=None\n",
        "#     for pred_id in range(len(y_pred[test_id])):\n",
        "#         if y_pred[test_id][pred_id][0]=='B':\n",
        "#             start_pos=pos\n",
        "#             entity_type=y_pred[test_id][pred_id][2:]\n",
        "#         elif start_pos is not None and y_pred[test_id][pred_id][0]=='I' and y_pred[test_id][pred_id+1][0]=='O':\n",
        "#             end_pos=pos\n",
        "#             entity_text=''.join([testdata_list[test_id][position][0] for position in range(start_pos,end_pos+1)])\n",
        "#             line=str(testdata_article_id_list[test_id])+'\\t'+str(start_pos)+'\\t'+str(end_pos+1)+'\\t'+entity_text+'\\t'+entity_type\n",
        "#             output+=line+'\\n'\n",
        "#         pos+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40HnTWGtRrnX"
      },
      "source": [
        "# output_path='output.tsv'\n",
        "# with open(output_path,'w',encoding='utf-8') as f:\n",
        "#     f.write(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-M36pQhxRrnX"
      },
      "source": [
        "# print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOuPLXj8RrnX"
      },
      "source": [
        "-----------------------------------------------------"
      ]
    }
  ]
}